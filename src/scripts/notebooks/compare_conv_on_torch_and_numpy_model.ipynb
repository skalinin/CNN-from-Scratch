{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d56a14c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from cnn.numpy_model import Conv2d\n",
    "from scripts.make_init_weights import (\n",
    "    conv_weights_converter, conv_biases_converter\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af7404f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_conv2d(kernel_size, stride, kernel_center, padding=0):\n",
    "    # random input array\n",
    "    input_array = np.random.rand(1, 5, 5).astype(np.float32)\n",
    "    \n",
    "    # torch Conv2d\n",
    "    torch_conv2d = torch.nn.Conv2d(1, 1, kernel_size, stride, padding=padding)\n",
    "    print(\"torch Conv2d:\")\n",
    "    input_tensor = torch.from_numpy(input_array).unsqueeze(0)\n",
    "    print(torch_conv2d(input_tensor))\n",
    "\n",
    "    # numpy implementation of Conv2d\n",
    "    numpy_conv2d = Conv2d(\n",
    "        kernel_size=kernel_size,\n",
    "        in_channels=1,\n",
    "        out_channels=1,\n",
    "        stride=stride,\n",
    "        kernel_center=kernel_center,\n",
    "        padding=padding,\n",
    "        convolution=False\n",
    "    )\n",
    "    # convert weights from torch conv2d to numpy conv2d\n",
    "    numpy_conv2d.conv_w = conv_weights_converter(torch_conv2d.state_dict()['weight'])\n",
    "    numpy_conv2d.conv_b = conv_biases_converter(torch_conv2d.state_dict()['bias'])\n",
    "\n",
    "    print(\"\\nNumpy implementation of Conv2d:\")\n",
    "    print(numpy_conv2d(input_array))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67da67a3",
   "metadata": {},
   "source": [
    "## Compare torch Conv2d to numpy-implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21574b24",
   "metadata": {},
   "source": [
    "#### Kernel center is always in (0, 0) position"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ae0acf",
   "metadata": {},
   "source": [
    "### Kernel size (2, 2) and stride 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9d603b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch Conv2d:\n",
      "tensor([[[[-0.2998, -0.3680],\n",
      "          [-0.5606, -0.4098]]]], grad_fn=<ThnnConv2DBackward>)\n",
      "\n",
      "Numpy implementation of Conv2d:\n",
      "[array([[-0.29976724, -0.36797103],\n",
      "       [-0.56061949, -0.40979104]])]\n"
     ]
    }
   ],
   "source": [
    "compare_conv2d(\n",
    "    kernel_size=(2, 2),\n",
    "    stride=2,\n",
    "    kernel_center=(0, 0)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02243e50",
   "metadata": {},
   "source": [
    "### Kernel size (3, 3) and stride 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "106618dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch Conv2d:\n",
      "tensor([[[[ 0.4767,  0.1116,  0.0358],\n",
      "          [ 0.3911,  0.0581,  0.4450],\n",
      "          [ 0.2346, -0.0311,  0.0658]]]], grad_fn=<ThnnConv2DBackward>)\n",
      "\n",
      "Numpy implementation of Conv2d:\n",
      "[array([[ 0.47665877,  0.11160256,  0.03579822],\n",
      "       [ 0.39114232,  0.05808807,  0.44496372],\n",
      "       [ 0.23458593, -0.03111989,  0.0658099 ]])]\n"
     ]
    }
   ],
   "source": [
    "compare_conv2d(\n",
    "    kernel_size=(3, 3),\n",
    "    stride=1,\n",
    "    kernel_center=(0, 0)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dcef28e",
   "metadata": {},
   "source": [
    "### Kernel size (4, 4) and stride 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "22dd3247",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch Conv2d:\n",
      "tensor([[[[0.4226]]]], grad_fn=<MkldnnConvolutionBackward>)\n",
      "\n",
      "Numpy implementation of Conv2d:\n",
      "[array([[0.42262959]])]\n"
     ]
    }
   ],
   "source": [
    "compare_conv2d(\n",
    "    kernel_size=(4, 4),\n",
    "    stride=2,\n",
    "    kernel_center=(0, 0)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09284693",
   "metadata": {},
   "source": [
    "### Kernel size (3, 3), stride 2, padding 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "293a210b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch Conv2d:\n",
      "tensor([[[[0.2034, 0.2034, 0.2034, 0.2034, 0.2034],\n",
      "          [0.2034, 0.3960, 0.6389, 0.4219, 0.2034],\n",
      "          [0.2034, 0.5173, 1.2061, 0.8548, 0.2034],\n",
      "          [0.2034, 0.6608, 0.8985, 0.7719, 0.2034],\n",
      "          [0.2034, 0.2034, 0.2034, 0.2034, 0.2034]]]],\n",
      "       grad_fn=<ThnnConv2DBackward>)\n",
      "\n",
      "Numpy implementation of Conv2d:\n",
      "[array([[0.20339541, 0.20339541, 0.20339541, 0.20339541, 0.20339541],\n",
      "       [0.20339541, 0.39604704, 0.63892502, 0.42186425, 0.20339541],\n",
      "       [0.20339541, 0.5173453 , 1.20608855, 0.85484172, 0.20339541],\n",
      "       [0.20339541, 0.66082276, 0.89851127, 0.77189236, 0.20339541],\n",
      "       [0.20339541, 0.20339541, 0.20339541, 0.20339541, 0.20339541]])]\n"
     ]
    }
   ],
   "source": [
    "compare_conv2d(\n",
    "    kernel_size=(3, 3),\n",
    "    stride=2,\n",
    "    kernel_center=(0, 0),\n",
    "    padding=3\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
